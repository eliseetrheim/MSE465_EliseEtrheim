{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b11b18",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NU-MSE-LECTURES/465-WINTER2026/blob/main/Week_04/assignments/assignment_04_combined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# MAT_SCI 465: Advanced Electron Microscopy & Diffraction\n",
    "## Week 03 & 04 Combined Assignment\n",
    "**Classical, ML, and Deep Learning Approaches to Microscopy Analysis**\n",
    "\n",
    "**Dataset:** DOPAD (Dataset Of nanoPArticle Detection) - TEM nanoparticle images for detection and classification  \n",
    "**Dataset Source:** https://dopad.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15333e3d",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Compare classical computer vision, machine learning, and deep learning approaches on the same electron microscopy dataset. Progress from traditional image processing through supervised and unsupervised learning to modern deep learning, enabling direct quantitative comparison of all methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d011bac",
   "metadata": {},
   "source": [
    "## Task 1 · Classical Image Analysis Pipeline\n",
    "- [ ] Apply noise reduction (Gaussian, median, or FFT filtering) and compute signal-to-noise ratio before and after using $SNR = u / igma$.\n",
    "- [ ] Enhance contrast via histogram equalization or CLAHE (clip limit 0.01-0.03).\n",
    "- [ ] Segment features using Otsu thresholding followed by Watershed to separate touching particles.\n",
    "- [ ] Quantify morphology with `regionprops` (area, diameter, eccentricity, solidity) and export measurements to `classical_results.csv`.\n",
    "- [ ] Create a four-panel figure: raw image, filtered/enhanced result, segmented labels, particle size distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a09f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/robertoreis/Documents/codes/465_Computational_Microscopy_2026/Week_04/assignments\n",
      "IMAGE_DIR: /Users/robertoreis/Documents/codes/465_Computational_Microscopy_2026/Week_04/assignments/raw_data\n",
      "IMAGE_DIR exists: True\n",
      "Loaded: 11500X00.png, Shape: (416, 416, 4)\n",
      "Total images available: 201\n"
     ]
    }
   ],
   "source": [
    "# Task 1 setup: imports and data loading\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import exposure, filters, measure, morphology, segmentation\n",
    "from skimage.io import imread\n",
    "from scipy import fft\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# DOPAD dataset: download from https://dopad.github.io/docs/download/\n",
    "# After extraction, adjust IMAGE_DIR to your local path\n",
    "\n",
    "# Use absolute path to ensure it works regardless of kernel working directory\n",
    "IMAGE_DIR = Path('/Users/robertoreis/Documents/codes/465_Computational_Microscopy_2026/Week_04/assignments/raw_data/')\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"IMAGE_DIR: {IMAGE_DIR}\")\n",
    "print(f\"IMAGE_DIR exists: {IMAGE_DIR.exists()}\")\n",
    "\n",
    "# Example: load a TEM image from DOPAD\n",
    "# For PNG/standard formats:\n",
    "if IMAGE_DIR.exists():\n",
    "    sample_images = sorted(list(IMAGE_DIR.glob('*.png')) + list(IMAGE_DIR.glob('*.tif')))\n",
    "    if sample_images:\n",
    "        raw_image = imread(str(sample_images[0]))\n",
    "        print(f\"Loaded: {sample_images[0].name}, Shape: {raw_image.shape}\")\n",
    "        print(f\"Total images available: {len(sample_images)}\")\n",
    "    else:\n",
    "        print(\"No images found in DOPAD directory\")\n",
    "else:\n",
    "    raw_image = None\n",
    "    print(\"IMAGE_DIR path not found. Please download DOPAD dataset and update path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dab0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 processing: filtering, enhancement, segmentation, quantification\n",
    "# HINTS for Task 1:\n",
    "# 1. Filtering: Use filters.median() or filters.gaussian() from skimage.filters\n",
    "#    Compare SNR before and after filtering to see improvement\n",
    "# 2. Enhancement: Use exposure.equalize_adapthist() with clip_limit around 0.025\n",
    "#    This will increase contrast without oversaturation\n",
    "# 3. Segmentation: After Otsu thresholding, use scipy.ndimage.distance_transform_edt()\n",
    "#    to find marker positions for Watershed\n",
    "# 4. Regionprops: Pass intensity_image parameter to get min/max/mean intensity\n",
    "#    Example: measure.regionprops(labels, intensity_image=enhanced_image)\n",
    "# 5. Export: Use pd.DataFrame.to_csv() to save measurements\n",
    "\n",
    "def compute_snr(image: np.ndarray) -> float:\n",
    "    signal = np.mean(image)\n",
    "    noise = np.std(image)\n",
    "    return float(signal / noise) if noise else np.inf\n",
    "\n",
    "# Filtering hint: Try median filter for salt-and-pepper noise\n",
    "# filtered_image = filters.median(raw_image, footprint=morphology.disk(3))\n",
    "filtered_image = None\n",
    "\n",
    "# Enhancement hint: CLAHE improves local contrast\n",
    "# enhanced_image = exposure.equalize_adapthist(filtered_image, clip_limit=0.025)\n",
    "enhanced_image = None\n",
    "\n",
    "# Segmentation hint: Use Otsu's threshold followed by watershed\n",
    "# threshold = filters.threshold_otsu(enhanced_image)\n",
    "# binary = enhanced_image > threshold\n",
    "# Then use distance transform + watershed to separate particles\n",
    "labels = None\n",
    "\n",
    "# Morphology hint: Extract area, perimeter, eccentricity, solidity, equivalent_diameter\n",
    "# Store in a DataFrame, then save with measurements.to_csv()\n",
    "measurements = pd.DataFrame()\n",
    "measurements.to_csv('classical_results.csv', index=False)\n",
    "\n",
    "# Visualization hint: Create 2x2 subplot figure with:\n",
    "# [Raw image] [Enhanced/Filtered]\n",
    "# [Segmented labels] [Size distribution histogram]\n",
    "# Use imshow() for images and hist() for distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea3156",
   "metadata": {},
   "source": [
    "## Task 2 · Machine Learning Approaches\n",
    "- [ ] Extract hand-crafted features: edges (Canny or Sobel), blobs (LoG), textures (LBP or GLCM) to build a feature matrix with at least 10 descriptors per region.\n",
    "- [ ] Perform feature selection using Random Forest importance or correlation analysis; retain the top 5-7 discriminative features.\n",
    "- [ ] Supervised: label data into two or more classes (≥50 samples), train SVM and Random Forest, then report precision, recall, F1-score, and confusion matrices.\n",
    "- [ ] Unsupervised: run k-Means (k ∈ {3, 5, 7}) and visualize using PCA or t-SNE; compute silhouette scores.\n",
    "- [ ] Compare ML outputs with classical segmentation regarding particle counts, accuracy, and runtime; export to `ml_results.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: feature extraction and ML pipelines\n",
    "from skimage.feature import canny, local_binary_pattern\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# HINTS for Task 2:\n",
    "# 1. Feature extraction: For each detected region, compute:\n",
    "#    - Area, perimeter, equivalent diameter, eccentricity, solidity (from regionprops)\n",
    "#    - Mean and std intensity (intensity-based features)\n",
    "#    - Edge features: number of edges detected by Canny filter\n",
    "#    - Texture: Local Binary Pattern variance\n",
    "#    - Circularity = 4*pi*area / perimeter^2\n",
    "#    - Total: 10+ features per region\n",
    "#\n",
    "# 2. Feature selection: Train a RandomForestClassifier and get feature_importances_\n",
    "#    Keep top 5-7 features with highest importance scores\n",
    "#    Hint: importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "#\n",
    "# 3. Supervised ML:\n",
    "#    - Split data into train/test (80/20)\n",
    "#    - Scale features using StandardScaler()\n",
    "#    - Train SVM with kernel='rbf' and Random Forest with n_estimators=100\n",
    "#    - Compute F1-score: metrics.f1_score(y_true, y_pred)\n",
    "#    - Create confusion matrix: metrics.confusion_matrix(y_true, y_pred)\n",
    "#\n",
    "# 4. Unsupervised ML:\n",
    "#    - Run KMeans for k in [3, 5, 7]\n",
    "#    - Compute silhouette_score(X_scaled, clusters) for each k\n",
    "#    - Use PCA(n_components=2) to project to 2D for visualization\n",
    "#    - Plot scatter with cluster labels as colors\n",
    "#\n",
    "# 5. Export: Save results to ml_results.csv with columns:\n",
    "#    [Method, F1-Score, Precision, Recall, Silhouette_Score]\n",
    "\n",
    "# TODO: assemble feature matrix and labels\n",
    "features = pd.DataFrame()\n",
    "labels = pd.Series(dtype=int)\n",
    "\n",
    "# TODO: perform feature selection and train supervised models\n",
    "svm_model = None\n",
    "rf_model = None\n",
    "\n",
    "# Feature selection hint:\n",
    "# rf_importance = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_importance.fit(X, y)\n",
    "# top_features = np.argsort(rf_importance.feature_importances_)[-7:]\n",
    "\n",
    "# TODO: evaluate models, generate confusion matrices, export results\n",
    "ml_results = pd.DataFrame()\n",
    "ml_results.to_csv('ml_results.csv', index=False)\n",
    "\n",
    "# Confusion matrix visualization hint:\n",
    "# Use seaborn.heatmap(confusion_matrix, annot=True) for nice heatmap display\n",
    "\n",
    "# TODO: run unsupervised clustering and visualize embeddings\n",
    "# K-Means hint: clusters = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n",
    "# PCA visualization: X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
    "#                   plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c96a1",
   "metadata": {},
   "source": [
    "## Task 3 · Deep Learning and Final Comparison\n",
    "- [ ] Prepare pixel-level annotations for 15-20 images and implement data augmentation (rotation, flips, zoom, intensity shifts, noise, etc.; 5+ variants).\n",
    "- [ ] CNN: build a compact convolutional network with 2-3 conv blocks, pooling, dropout, and dense heads; train, plot learning curves, and report F1-score versus classical ML.\n",
    "- [ ] U-Net: implement encoder-decoder with skip connections, train with Dice or BCE loss, evaluate IoU and Dice, and visualize intermediate feature maps.\n",
    "- [ ] Develop a comparison table covering method, accuracy/F1/IoU, runtime, data requirements, and interpretability (include Watershed, SVM, Random Forest, k-Means, CNN, U-Net).\n",
    "- [ ] Generate final 3×3 visualization panels summarizing the full pipeline and document findings.\n",
    "- [ ] Author README.md with methodology, quantitative comparison, recommended use-cases; export publication-quality figures with scale bars and submit repository URL on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43801f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: deep learning workflow templates\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# HINTS for Task 3:\n",
    "# 1. Data preparation:\n",
    "#    - Use ImageDataGenerator with augmentation parameters:\n",
    "#      rotation_range=30, width_shift_range=0.2, height_shift_range=0.2,\n",
    "#      zoom_range=0.2, fill_mode='reflect'\n",
    "#    - Normalize pixel values to [0, 1]\n",
    "#    - Use flow_from_directory() or flow() from keras preprocessing\n",
    "#\n",
    "# 2. CNN Architecture (simple but effective):\n",
    "#    - Conv2D(32, 3x3) + BatchNorm + ReLU + MaxPool(2x2)\n",
    "#    - Conv2D(64, 3x3) + BatchNorm + ReLU + MaxPool(2x2)\n",
    "#    - Flatten + Dense(128) + Dropout(0.5) + Dense(num_classes)\n",
    "#    - Compile with optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\n",
    "#    - Train with model.fit(train_dataset, validation_data=val_dataset, epochs=20)\n",
    "#\n",
    "# 3. U-Net Architecture (for segmentation):\n",
    "#    - Encoder: Conv -> MaxPool (downsampling)\n",
    "#    - Decoder: UpSampling -> Conv (upsampling)\n",
    "#    - Skip connections: concatenate encoder features with decoder features\n",
    "#    - Output: single channel with sigmoid activation (binary segmentation)\n",
    "#    - Loss: keras.losses.BinaryCrossentropy() or Dice loss\n",
    "#\n",
    "# 4. Evaluation metrics:\n",
    "#    - CNN: F1-score = 2 * (precision * recall) / (precision + recall)\n",
    "#    - U-Net: IoU = intersection / union, Dice = 2 * intersection / (A + B)\n",
    "#    - Hint: Use sklearn.metrics.f1_score, precision_score, recall_score\n",
    "#\n",
    "# 5. Visualization:\n",
    "#    - Plot training curves: plt.plot(history.history['loss'], label='train')\n",
    "#    - Show ground truth vs predictions side by side\n",
    "#    - Display intermediate feature maps from encoder layers\n",
    "\n",
    "# TODO: build data loaders with augmentation pipelines\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "\n",
    "# Data augmentation hint:\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=30,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     fill_mode='reflect'\n",
    "# )\n",
    "# train_dataset = datagen.flow_from_directory('train_path', target_size=(416, 416))\n",
    "\n",
    "# TODO: define CNN model\n",
    "cnn_model = None\n",
    "\n",
    "# CNN model hint:\n",
    "# model = keras.Sequential([\n",
    "#     layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(416, 416, 1)),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.MaxPooling2D(2),\n",
    "#     layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.MaxPooling2D(2),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# TODO: train CNN and plot metrics\n",
    "cnn_history = None\n",
    "\n",
    "# Training hint:\n",
    "# cnn_history = cnn_model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=val_dataset,\n",
    "#     epochs=20,\n",
    "#     callbacks=[keras.callbacks.EarlyStopping(patience=3)]\n",
    "# )\n",
    "\n",
    "# TODO: define U-Net architecture\n",
    "unet_model = None\n",
    "\n",
    "# U-Net hint: Build encoder (downsampling) then decoder (upsampling) with skip connections\n",
    "# Encoder:  Conv -> ReLU -> MaxPool (4 levels)\n",
    "# Decoder:  UpSampling -> Conv + concatenate from encoder\n",
    "# Output:   Conv(1, 1x1, sigmoid) for binary segmentation\n",
    "\n",
    "# TODO: train U-Net, compute IoU and Dice, visualize feature maps\n",
    "# U-Net metrics hint:\n",
    "# y_pred = unet_model.predict(X_test)\n",
    "# iou = jaccard_score(y_test.flatten(), (y_pred > 0.5).flatten())\n",
    "# dice = f1_score(y_test.flatten(), (y_pred > 0.5).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40132eef",
   "metadata": {},
   "source": [
    "## Dataset: DOPAD (Dataset Of nanoPArticle Detection)\n",
    "\n",
    "**Overview:**\n",
    "- **272 original TEM images** at varying resolutions (~1.5M total particles)\n",
    "- High-resolution nanoparticle detection annotations\n",
    "- Diverse imaging environments and particle morphologies\n",
    "\n",
    "**Note:** You do **not** need to use all 272 images. Using **100images** is sufficient for this assignment and will provide excellent statistical validation while keeping computational cost manageable.\n",
    "\n",
    "**Download:** https://dopad.github.io/docs/download/\n",
    "\n",
    "**Citation:** Qu et al. - For academic use, cite appropriately per repository guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a125adb",
   "metadata": {},
   "source": [
    "## Deliverables Checklist\n",
    "- [ ] Classical pipeline outputs (`classical_results.csv`, four-panel figure).\n",
    "- [ ] ML analyses (`ml_results.csv`, confusion matrices, clustering visualizations).\n",
    "- [ ] Deep learning artifacts (training curves, segmentation outputs, feature maps).\n",
    "- [ ] Comparison table summarizing methods and metrics.\n",
    "- [ ] Final 3×3 visualization collage.\n",
    "- [ ] README.md documenting methodology, quantitative comparisons, and recommendations.\n",
    "- [ ] Publication-quality figures with scale bars and repository submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matsci465",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
